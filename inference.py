"""
æ¨ç†è„šæœ¬ - å¸¦æœ‰å…¨é¢çš„è¯„ä¼°æŒ‡æ ‡
ç”¨äºåŠ è½½è®­ç»ƒå¥½çš„æ¨¡å‹è¿›è¡Œæ‰‹è¯­ç¿»è¯‘æ¨ç†
"""
import torch
import os
import argparse
import sys
from tqdm import tqdm
from collections import Counter

sys.path.insert(0, os.path.dirname(os.path.abspath(__file__)))

from config_lite import InferenceConfig, mt5_path, test_label_path, pose_dir
from models_lite import SignLanguageLite
from datasets_lite import SignLanguageDataset
from torch.utils.data import DataLoader


def load_model(model_path, config):
    """
    åŠ è½½è®­ç»ƒå¥½çš„æ¨¡å‹
    
    Args:
        model_path: æ¨¡å‹æƒé‡æ–‡ä»¶è·¯å¾„
        config: é…ç½®å¯¹è±¡
    
    Returns:
        model: åŠ è½½å¥½çš„æ¨¡å‹
    """
    class Args:
        pass
    
    args = Args()
    args.mt5_path = mt5_path
    args.max_length = config.max_length
    args.label_smoothing = 0  # æ¨ç†æ—¶ä¸éœ€è¦æ ‡ç­¾å¹³æ»‘
    
    # è‡ªåŠ¨æ¨æ–­ Gloss è¯è¡¨å¤§å° (Fix for shape mismatch)
    gloss_vocab_path = os.path.join(os.path.dirname(model_path), 'gloss_vocab.json')
    if not os.path.exists(gloss_vocab_path):
        # å°è¯•é»˜è®¤è·¯å¾„
        gloss_vocab_path = os.path.join(os.path.dirname(os.path.abspath(__file__)), 'checkpoints', 'gloss_vocab.json')
        
    if os.path.exists(gloss_vocab_path):
        try:
            import json
            with open(gloss_vocab_path, 'r', encoding='utf-8') as f:
                vocab_data = json.load(f)
                # å¦‚æœæ˜¯ word2idx ç»“æ„
                if 'word2idx' in vocab_data:
                    args.gloss_vocab_size = len(vocab_data['word2idx'])
                else:
                    args.gloss_vocab_size = len(vocab_data)
                print(f"å·²åŠ è½½ Gloss è¯è¡¨ï¼Œå¤§å°: {args.gloss_vocab_size}")
        except Exception as e:
            print(f"æ— æ³•åŠ è½½ Gloss è¯è¡¨: {e}ï¼Œä½¿ç”¨é»˜è®¤å¤§å° 2000")
            args.gloss_vocab_size = 2000
    else:
        print("æœªæ‰¾åˆ° Gloss è¯è¡¨æ–‡ä»¶ï¼Œä½¿ç”¨é»˜è®¤å¤§å° 2000")
        args.gloss_vocab_size = 2000

    model = SignLanguageLite(args)
    
    if os.path.exists(model_path):
        print(f"åŠ è½½æ¨¡å‹æƒé‡: {model_path}")
        state_dict = torch.load(model_path, map_location='cpu', weights_only=True)
        model.load_state_dict(state_dict)
    else:
        print(f"è­¦å‘Š: æ¨¡å‹æ–‡ä»¶ä¸å­˜åœ¨ {model_path}")
        print("å°†ä½¿ç”¨æœªè®­ç»ƒçš„æ¨¡å‹")
    
    model = model.to(config.device)
    model.eval()
    
    return model


def compute_bleu(reference, hypothesis, max_n=4):
    """
    è®¡ç®—BLEUåˆ†æ•°ï¼ˆç®€åŒ–ç‰ˆï¼ŒåŸºäºå­—ç¬¦çº§åˆ«ï¼‰
    
    Args:
        reference: å‚è€ƒæ–‡æœ¬
        hypothesis: ç”Ÿæˆæ–‡æœ¬
        max_n: æœ€å¤§n-gram
    
    Returns:
        bleu_score: BLEUåˆ†æ•°
    """
    import math
    
    ref_chars = list(reference.strip())
    hyp_chars = list(hypothesis.strip())
    
    if len(hyp_chars) == 0:
        return 0.0
    
    # è®¡ç®—å„é˜¶n-gramç²¾ç¡®ç‡
    precisions = []
    for n in range(1, min(max_n + 1, len(hyp_chars) + 1)):
        # å‚è€ƒæ–‡æœ¬çš„n-gramè®¡æ•°
        ref_ngrams = Counter()
        for i in range(len(ref_chars) - n + 1):
            ngram = tuple(ref_chars[i:i+n])
            ref_ngrams[ngram] += 1
        
        # å‡è®¾æ–‡æœ¬çš„n-gramè®¡æ•°
        hyp_ngrams = Counter()
        for i in range(len(hyp_chars) - n + 1):
            ngram = tuple(hyp_chars[i:i+n])
            hyp_ngrams[ngram] += 1
        
        # è®¡ç®—clippedè®¡æ•°
        clipped_count = 0
        total_count = 0
        for ngram, count in hyp_ngrams.items():
            clipped_count += min(count, ref_ngrams.get(ngram, 0))
            total_count += count
        
        if total_count > 0:
            precisions.append(clipped_count / total_count)
        else:
            precisions.append(0)
    
    if not precisions or all(p == 0 for p in precisions):
        return 0.0
    
    # å‡ ä½•å¹³å‡
    log_precision = sum(math.log(p) if p > 0 else -float('inf') for p in precisions) / len(precisions)
    
    # ç®€çŸ­æƒ©ç½š
    bp = 1.0
    if len(hyp_chars) < len(ref_chars):
        bp = math.exp(1 - len(ref_chars) / len(hyp_chars))
    
    bleu = bp * math.exp(log_precision) if log_precision > -float('inf') else 0.0
    
    return bleu


def compute_edit_distance(s1, s2):
    """è®¡ç®—ç¼–è¾‘è·ç¦»"""
    m, n = len(s1), len(s2)
    dp = [[0] * (n + 1) for _ in range(m + 1)]
    
    for i in range(m + 1):
        dp[i][0] = i
    for j in range(n + 1):
        dp[0][j] = j
    
    for i in range(1, m + 1):
        for j in range(1, n + 1):
            if s1[i-1] == s2[j-1]:
                dp[i][j] = dp[i-1][j-1]
            else:
                dp[i][j] = min(dp[i-1][j], dp[i][j-1], dp[i-1][j-1]) + 1
    
    return dp[m][n]


def compute_wer(reference, hypothesis):
    """
    è®¡ç®—è¯é”™è¯¯ç‡ (WER) - å¯¹ä¸­æ–‡ä½¿ç”¨å­—ç¬¦çº§åˆ«
    
    Args:
        reference: å‚è€ƒæ–‡æœ¬
        hypothesis: ç”Ÿæˆæ–‡æœ¬
    
    Returns:
        wer: è¯é”™è¯¯ç‡ (0-1, è¶Šä½è¶Šå¥½)
    """
    ref_chars = list(reference.strip())
    hyp_chars = list(hypothesis.strip())
    
    if len(ref_chars) == 0:
        return 1.0 if len(hyp_chars) > 0 else 0.0
    
    edit_dist = compute_edit_distance(ref_chars, hyp_chars)
    wer = edit_dist / len(ref_chars)
    
    return min(wer, 1.0)  # é™åˆ¶åœ¨0-1èŒƒå›´å†…


def compute_accuracy_metrics(predictions, ground_truths):
    """
    è®¡ç®—å¤šç§å‡†ç¡®ç‡æŒ‡æ ‡
    
    Args:
        predictions: é¢„æµ‹æ–‡æœ¬åˆ—è¡¨
        ground_truths: çœŸå®æ–‡æœ¬åˆ—è¡¨
    
    Returns:
        metrics: åŒ…å«å¤šç§æŒ‡æ ‡çš„å­—å…¸
    """
    exact_match = 0
    char_correct = 0
    char_total = 0
    total_bleu = 0
    total_wer = 0
    
    # éƒ¨åˆ†åŒ¹é…ç»Ÿè®¡
    partial_50 = 0  # 50%ä»¥ä¸Šå­—ç¬¦åŒ¹é…
    partial_80 = 0  # 80%ä»¥ä¸Šå­—ç¬¦åŒ¹é…
    
    for pred, gt in zip(predictions, ground_truths):
        pred = pred.strip()
        gt = gt.strip()
        
        # 1. å®Œå…¨åŒ¹é…
        if pred == gt:
            exact_match += 1
        
        # 2. å­—ç¬¦çº§å‡†ç¡®ç‡
        match_count = 0
        max_len = max(len(pred), len(gt))
        min_len = min(len(pred), len(gt))
        
        for i in range(min_len):
            if pred[i] == gt[i]:
                match_count += 1
                char_correct += 1
            char_total += 1
        
        # è¡¥é½é•¿åº¦å·®å¼‚
        char_total += abs(len(pred) - len(gt))
        
        # è®¡ç®—è¯¥æ ·æœ¬çš„å­—ç¬¦åŒ¹é…ç‡
        if len(gt) > 0:
            char_match_rate = match_count / len(gt)
            if char_match_rate >= 0.5:
                partial_50 += 1
            if char_match_rate >= 0.8:
                partial_80 += 1
        
        # 3. BLEUåˆ†æ•°
        bleu = compute_bleu(gt, pred)
        total_bleu += bleu
        
        # 4. WER
        wer = compute_wer(gt, pred)
        total_wer += wer
    
    n = len(predictions)
    
    metrics = {
        'exact_match': exact_match / n if n > 0 else 0,
        'char_accuracy': char_correct / char_total if char_total > 0 else 0,
        'partial_50': partial_50 / n if n > 0 else 0,  # 50%ä»¥ä¸ŠåŒ¹é…
        'partial_80': partial_80 / n if n > 0 else 0,  # 80%ä»¥ä¸ŠåŒ¹é…
        'bleu': total_bleu / n if n > 0 else 0,
        'wer': total_wer / n if n > 0 else 0,
        'total_samples': n,
        'exact_match_count': exact_match,
    }
    
    return metrics


def inference_batch(model, dataloader, config, output_file=None):
    """
    æ‰¹é‡æ¨ç†
    
    Args:
        model: æ¨¡å‹
        dataloader: æ•°æ®åŠ è½½å™¨
        config: é…ç½®å¯¹è±¡
        output_file: è¾“å‡ºæ–‡ä»¶è·¯å¾„ (å¯é€‰)
    
    Returns:
        results: æ¨ç†ç»“æœåˆ—è¡¨
    """
    results = []
    
    model.eval()
    
    with torch.no_grad():
        for src_input, tgt_input in tqdm(dataloader, desc="æ¨ç†ä¸­"):
            # ç§»åŠ¨åˆ°è®¾å¤‡
            for key in ['body', 'left', 'right', 'face', 'attention_mask']:
                if key in src_input:
                    src_input[key] = src_input[key].to(config.device)
            
            # ç”Ÿæˆç¿»è¯‘
            predictions = model.generate(
                src_input, 
                max_new_tokens=config.max_new_tokens
            )
            
            # æ”¶é›†ç»“æœ
            names = src_input.get('names', ['unknown'] * len(predictions))
            gt_sentences = tgt_input.get('gt_sentence', [''] * len(predictions))
            
            for name, pred, gt in zip(names, predictions, gt_sentences):
                results.append({
                    'name': name,
                    'prediction': pred,
                    'ground_truth': gt
                })
    
    # ä¿å­˜ç»“æœ
    if output_file:
        with open(output_file, 'w', encoding='utf-8') as f:
            for r in results:
                f.write(f"åç§°: {r['name']}\n")
                f.write(f"é¢„æµ‹: {r['prediction']}\n")
                f.write(f"çœŸå®: {r['ground_truth']}\n")
                f.write("-" * 50 + "\n")
        print(f"ç»“æœå·²ä¿å­˜åˆ°: {output_file}")
    
    return results


def print_evaluation_report(metrics, results, num_samples=10):
    """æ‰“å°è¯¦ç»†çš„è¯„ä¼°æŠ¥å‘Š"""
    print("\n" + "=" * 60)
    print("è¯„ä¼°æŠ¥å‘Š")
    print("=" * 60)
    
    print(f"\nğŸ“Š æ€»ä½“ç»Ÿè®¡:")
    print(f"  æ€»æ ·æœ¬æ•°: {metrics['total_samples']}")
    print(f"  å®Œå…¨åŒ¹é…: {metrics['exact_match_count']}")
    
    print(f"\nğŸ“ˆ å‡†ç¡®ç‡æŒ‡æ ‡:")
    print(f"  å®Œå…¨åŒ¹é…ç‡:   {metrics['exact_match']:.2%}")
    print(f"  å­—ç¬¦å‡†ç¡®ç‡:   {metrics['char_accuracy']:.2%}")
    print(f"  50%éƒ¨åˆ†åŒ¹é…: {metrics['partial_50']:.2%}")
    print(f"  80%éƒ¨åˆ†åŒ¹é…: {metrics['partial_80']:.2%}")
    
    print(f"\nğŸ“ å…¶ä»–æŒ‡æ ‡:")
    print(f"  BLEUåˆ†æ•°:     {metrics['bleu']:.4f}")
    print(f"  å­—ç¬¦é”™è¯¯ç‡:   {metrics['wer']:.2%} (è¶Šä½è¶Šå¥½)")
    
    print(f"\nğŸ“ é¢„æµ‹ç¤ºä¾‹ (å‰{num_samples}ä¸ª):")
    for i, r in enumerate(results[:num_samples]):
        pred = r['prediction']
        gt = r['ground_truth']
        
        # è®¡ç®—åŒ¹é…æƒ…å†µ
        match_chars = sum(1 for p, g in zip(pred, gt) if p == g)
        match_rate = match_chars / len(gt) if len(gt) > 0 else 0
        
        status = "âœ“" if pred == gt else f"({match_rate:.0%})"
        
        print(f"\n  [{i+1}] {status}")
        print(f"      é¢„æµ‹: {pred}")
        print(f"      çœŸå®: {gt}")
    
    print("\n" + "=" * 60)
    
    # åˆ†æå¸¸è§é”™è¯¯
    print("\nğŸ” é”™è¯¯åˆ†æ:")
    errors = [r for r in results if r['prediction'] != r['ground_truth']]
    
    if errors:
        # ç»Ÿè®¡é¢„æµ‹é•¿åº¦åå·®
        length_diffs = [len(r['prediction']) - len(r['ground_truth']) for r in errors]
        avg_diff = sum(length_diffs) / len(length_diffs) if length_diffs else 0
        
        print(f"  é”™è¯¯æ ·æœ¬æ•°: {len(errors)}")
        print(f"  é¢„æµ‹é•¿åº¦å¹³å‡åå·®: {avg_diff:+.1f} å­—ç¬¦")
        
        # ç»Ÿè®¡æ˜¯å¦æœ‰ç©ºé¢„æµ‹
        empty_preds = sum(1 for r in results if len(r['prediction'].strip()) == 0)
        if empty_preds > 0:
            print(f"  ç©ºé¢„æµ‹æ•°é‡: {empty_preds}")
    else:
        print("  æ— é”™è¯¯ï¼æ‰€æœ‰é¢„æµ‹å®Œå…¨åŒ¹é…ï¼")
    
    print("=" * 60)


def main():
    parser = argparse.ArgumentParser(description='æ‰‹è¯­ç¿»è¯‘æ¨ç†ä¸è¯„ä¼°')
    parser.add_argument('--model_path', type=str, default='checkpoints/best_model.pth',
                        help='æ¨¡å‹æƒé‡æ–‡ä»¶è·¯å¾„')
    parser.add_argument('--label_path', type=str, default=None,
                        help='æ ‡ç­¾æ–‡ä»¶è·¯å¾„ (é»˜è®¤ä½¿ç”¨æµ‹è¯•é›†)')
    parser.add_argument('--output', type=str, default='inference_results.txt',
                        help='è¾“å‡ºç»“æœæ–‡ä»¶è·¯å¾„')
    parser.add_argument('--batch_size', type=int, default=1,
                        help='æ‰¹é‡å¤§å°')
    parser.add_argument('--num_beams', type=int, default=4,
                        help='Beam search æ•°é‡')
    
    args = parser.parse_args()
    
    # é…ç½®
    config = InferenceConfig()
    config.batch_size = args.batch_size
    config.num_beams = args.num_beams
    
    # å¤„ç†æ¨¡å‹è·¯å¾„
    base_dir = os.path.dirname(os.path.abspath(__file__))
    model_path = args.model_path
    if not os.path.isabs(model_path):
        model_path = os.path.join(base_dir, model_path)
    
    # åŠ è½½æ¨¡å‹
    model = load_model(model_path, config)
    
    # åŠ è½½æ•°æ®
    label_path = args.label_path if args.label_path else test_label_path
    
    print(f"åŠ è½½æµ‹è¯•æ•°æ®: {label_path}")
    
    try:
        dataset = SignLanguageDataset(label_path, config, phase='test')
        
        if len(dataset) == 0:
            print("é”™è¯¯: æ•°æ®é›†ä¸ºç©º")
            return
        
        print(f"æµ‹è¯•æ ·æœ¬æ•°: {len(dataset)}")
        
        dataloader = DataLoader(
            dataset,
            batch_size=config.batch_size,
            shuffle=False,
            collate_fn=dataset.collate_fn
        )
        
        # æ¨ç†
        output_path = os.path.join(base_dir, args.output)
        results = inference_batch(model, dataloader, config, output_path)
        
        # è¯„ä¼°
        if results:
            predictions = [r['prediction'] for r in results]
            ground_truths = [r['ground_truth'] for r in results]
            
            metrics = compute_accuracy_metrics(predictions, ground_truths)
            print_evaluation_report(metrics, results)
            
    except Exception as e:
        print(f"æ¨ç†å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()


if __name__ == '__main__':
    main()
